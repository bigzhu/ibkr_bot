"""
ä¸»è°ƒåº¦é€»è¾‘ - çº¯å‡½æ•°å¼è°ƒåº¦å™¨å®ç° (ä»…è¿›ç¨‹æ± æ¨¡å¼)

åŠŸèƒ½:
1. æ•´åˆæ—¶é—´åŒ¹é…,é…ç½®æŸ¥è¯¢,DeMarkä¿¡å·å¤„ç†
2. æ¯åˆ†é’Ÿæ•´ç‚¹æ‰§è¡ŒDeMarkä¿¡å·æ£€æµ‹
3. ä½¿ç”¨ ProcessPoolExecutor å¤ç”¨æ‰§è¡Œå™¨, ä¸å†æ”¯æŒå­è¿›ç¨‹ stdout è§£æ

ç”¨æ³•:
    p main_scheduler.py  # å¯åŠ¨è°ƒåº¦å™¨
ç¯å¢ƒ:
    SCHEDULER_MAX_PROCESSES  é…ç½®è¿›ç¨‹æ± å¤§å°, é»˜è®¤ 14
    SCHEDULER_WORKER_LOG_LEVEL å­è¿›ç¨‹æ—¥å¿—çº§åˆ«, é»˜è®¤ WARNING
    SCHEDULER_LOG_STYLE      æ—¥å¿—é£æ ¼ concise|normal, é»˜è®¤ normal
"""

import asyncio
import concurrent.futures
import contextlib
import os
import sys
from datetime import UTC, datetime
from functools import partial
from pathlib import Path
from typing import Any, cast

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

# æ‰§è¡Œæ—¶é•¿ä¿æŠ¤: ä¿è¯æ‰¹æ¬¡èƒ½åœ¨ä¸€åˆ†é’Ÿå†…å®Œæˆ
PER_TASK_TIMEOUT_SECONDS = 50.0

from shared.logger_utils import setup_scheduler_logger

setup_scheduler_logger()

from loguru import logger

if __name__ == "__main__":
    try:
        from shared.path_utils import ensure_project_root_for_script
    except ImportError:
        sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
        from shared.path_utils import ensure_project_root_for_script

    ensure_project_root_for_script(__file__)

from database import get_database_manager
from database.db_config import get_default_database_config
from scheduler.config_loader import get_active_configs_by_timeframes
from scheduler.timeframe_matcher import get_matched_timeframes

# æ‰§è¡Œæ¨¡å¼: ä»…è¿›ç¨‹æ± 
MAX_PROCESSES = int(os.getenv("SCHEDULER_MAX_PROCESSES", "14"))

_process_pool: concurrent.futures.ProcessPoolExecutor | None = None
WORKER_LOG_LEVEL = os.getenv("SCHEDULER_WORKER_LOG_LEVEL", "WARNING").upper()
LOG_STYLE = os.getenv("SCHEDULER_LOG_STYLE", "normal").lower()


def _init_worker_logger() -> None:
    """è¿›ç¨‹æ± å­è¿›ç¨‹æ—¥å¿—åˆå§‹åŒ–: é»˜è®¤æŠ‘åˆ¶åˆ° WARNING, ä¿æŒä¸»è¿›ç¨‹è¾“å‡ºç®€æ´"""
    try:
        import sys as _wsys

        from loguru import logger as _wlogger

        _wlogger.remove()
        # ä½¿ç”¨è¾ƒé«˜ç­‰çº§ä»¥å‡å°‘å™ªéŸ³; æ ¼å¼æç®€
        _ = _wlogger.add(
            _wsys.stdout,
            level=WORKER_LOG_LEVEL,
            format="{message}",
        )
    except Exception:
        # å®‰é™å¤±è´¥, é¿å…å½±å“ä»»åŠ¡
        pass


"""ä»…è¿›ç¨‹æ± æ‰§è¡Œæ¨¡å¼; å­è¿›ç¨‹æ‰§è¡Œè·¯å¾„å·²ç§»é™¤"""


def _get_process_pool() -> concurrent.futures.ProcessPoolExecutor:
    global _process_pool
    if _process_pool is None:
        _process_pool = concurrent.futures.ProcessPoolExecutor(
            max_workers=MAX_PROCESSES,
            initializer=_init_worker_logger,
        )
        logger.info(f"å·²åˆå§‹åŒ–è¿›ç¨‹æ± , workers={MAX_PROCESSES}")
    return _process_pool


# å­è¿›ç¨‹æ‰§è¡Œæ¨¡å¼å·²ç§»é™¤; ç»Ÿä¸€ä½¿ç”¨è¿›ç¨‹æ± å¤ç”¨æ‰§è¡Œå™¨


def _create_signal_result(
    symbol: str,
    timeframe: str,
    config_id: int,
    action: str,
    signal_value: int,
    end_time: datetime,
    duration: float,
) -> dict[str, Any]:
    """åˆ›å»ºä¿¡å·å¤„ç†ç»“æœ"""
    return {
        "timestamp": end_time.isoformat(),
        "config_id": config_id,
        "symbol": symbol.upper(),
        "timeframe": timeframe,
        "success": True,
        "action": action,
        "signal_value": signal_value,
        "duration_seconds": round(duration, 3),
    }


async def process_demark_signal(
    symbol: str, timeframe: str, config_id: int
) -> dict[str, Any]:
    """
    å¤„ç†å•ä¸ªé…ç½®çš„è®¢å•æ„å»ºæµç¨‹

    Args:
        symbol: äº¤æ˜“å¯¹ç¬¦å·
        timeframe: æ—¶é—´å‘¨æœŸ
        config_id: é…ç½®ID

    Returns:
        è®¢å•æ„å»ºæ‰§è¡Œç»“æœ
    """
    start_time = datetime.now()

    # æ­¤å¤„ä¸åš monitor_delay ç­‰å¾…; ç”±ä¸‹æ¸¸æ•°æ®å±‚è‡ªè¡Œä¿è¯Kçº¿å®Œæ•´æ€§

    # ä»…ä½¿ç”¨è¿›ç¨‹æ± å¤ç”¨, é¿å…æ¯ä»»åŠ¡å†·å¯åŠ¨
    from order_builder.app import run_order_builder
    from shared.types.order_builder import OrderPlacedResult, RunResult

    loop = asyncio.get_running_loop()
    try:
        result_dict = await asyncio.wait_for(
            loop.run_in_executor(
                _get_process_pool(), partial(run_order_builder, symbol, timeframe)
            ),
            timeout=PER_TASK_TIMEOUT_SECONDS,
        )
    except TimeoutError:
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        timeout_result = _create_signal_result(
            symbol, timeframe, config_id, "TIMEOUT", 0, end_time, duration
        )
        logger.warning(
            "ä»»åŠ¡è¶…æ—¶(è¿›ç¨‹æ± ) {}-{} (>{}s)",
            symbol,
            timeframe,
            PER_TASK_TIMEOUT_SECONDS,
        )
        return timeout_result

    res_any: Any = result_dict
    if not isinstance(res_any, dict):
        raise RuntimeError("è®¢å•æ„å»ºè¿”å›ç»“æœæ— æ•ˆ")

    result_data = cast(RunResult, res_any)

    action = str(result_data.get("action", "UNKNOWN"))
    signal_value = int(result_data.get("signal_value", 0) or 0)
    extras: dict[str, Any] = {}
    if action == "ORDER_PLACED":
        placed = cast("OrderPlacedResult", result_data)
        extras["qty"] = placed["qty"]
        extras["price"] = placed["price"]
        extras["order_id"] = placed["order_id"]
    # ä¸ä»å­ç»“æœä¸­è¯»å–è€—æ—¶; è°ƒåº¦å™¨è‡ªè¡Œç»Ÿè®¡æ‰¹å†…è€—æ—¶

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    signal_result = _create_signal_result(
        symbol, timeframe, config_id, action, signal_value, end_time, duration
    )

    extra_part = ""
    if extras.get("order_id") is not None:
        extra_part = f", è®¢å•={extras.get('order_id')}, æ•°é‡={extras.get('qty')}, ä»·æ ¼={extras.get('price')}"
    if LOG_STYLE == "concise":
        info_msg = f"{symbol}-{timeframe} åŠ¨ä½œ={action} å€¼={signal_value}, è€—æ—¶={duration:.3f}s{extra_part}"
    else:
        info_msg = (
            f"âœ… å®Œæˆå¤„ç† {symbol}-{timeframe} - "
            f"è€—æ—¶: {duration:.3f}s, ç»“æœå€¼={signal_value}, åŠ¨ä½œ={action}{extra_part} - "
            f"{end_time.strftime('%H:%M:%S.%f')[:-3]}"
        )
    logger.info(info_msg)
    return signal_result


async def process_configs_concurrent(
    active_configs: list[dict[str, Any]],
) -> list[dict[str, Any]]:
    """
    å¹¶å‘å¤„ç†æ‰€æœ‰æ´»è·ƒé…ç½® - fail-fastç­–ç•¥

    Args:
        active_configs: æ´»è·ƒé…ç½®åˆ—è¡¨

    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨

    Note:
        éµå¾ªfail-faståŸåˆ™, ä»»ä½•é…ç½®å¤„ç†å¤±è´¥éƒ½ä¼šå¯¼è‡´æ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
        è¿™ç¡®ä¿æ•°æ®å®Œæ•´æ€§, é¿å…éƒ¨åˆ†æˆåŠŸçš„ä¸ä¸€è‡´çŠ¶æ€
    """
    if not active_configs:
        return []

    batch_start_time = datetime.now()

    # ä¿ç•™åŸè®¾è®¡: æ¯ä¸ªå­è¿›ç¨‹ç‹¬ç«‹åŒæ­¥ä¸æ’®åˆ,ä¸åšæ‰¹å‰é¢„åŒæ­¥

    # åˆ›å»ºå¹¶å‘ä»»åŠ¡ - ä½¿ç”¨asyncio.create_taskç¡®ä¿çœŸæ­£å¹¶å‘
    tasks: list[asyncio.Task[dict[str, Any]]] = []
    for config in active_configs:
        task = asyncio.create_task(
            process_demark_signal(
                config["trading_symbol"],
                config["kline_timeframe"],
                config["id"],
            )
        )
        tasks.append(task)

    # å¹¶å‘æ‰§è¡Œ - æ•è·å–æ¶ˆå¼‚å¸¸é¿å…ä¸­æ–­æ—¶çš„é”™è¯¯å †æ ˆ
    try:
        results_tuple = await asyncio.gather(*tasks)
        results: list[dict[str, Any]] = list(results_tuple)
    except asyncio.CancelledError:
        # ä»»åŠ¡è¢«å–æ¶ˆæ—¶,å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
        for task in tasks:
            if not task.done():
                _ = task.cancel()
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆæ¸…ç†
        _ = await asyncio.gather(*tasks, return_exceptions=True)
        raise

    batch_end_time = datetime.now()
    batch_duration = (batch_end_time - batch_start_time).total_seconds()

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_individual_time = sum(r.get("duration_seconds", 0) for r in results)
    parallelism_ratio = (
        total_individual_time / batch_duration if batch_duration > 0 else 0
    )

    if LOG_STYLE == "concise":
        summary_msg = f"batch è€—æ—¶={batch_duration:.3f}s, æ€»å’Œ={total_individual_time:.3f}s, å¹¶è¡Œ={parallelism_ratio:.1f}x"
    else:
        summary_msg = (
            f"ğŸ‰ å¹¶å‘å¤„ç†å®Œæˆ - æ€»è€—æ—¶: {batch_duration:.3f}s, "
            f"å•ç‹¬è€—æ—¶æ€»å’Œ: {total_individual_time:.3f}s, "
            f"å¹¶è¡Œæ•ˆç‡: {parallelism_ratio:.1f}x - "
            f"{batch_end_time.strftime('%H:%M:%S.%f')[:-3]}"
        )
    logger.info(summary_msg)

    return results


def _check_matched_timeframes(
    current_time: datetime,
) -> tuple[list[str], dict[str, Any] | None]:
    """æ£€æŸ¥åŒ¹é…çš„æ—¶é—´å‘¨æœŸ"""
    matched_timeframes = get_matched_timeframes(current_time)
    if not matched_timeframes:
        empty_timeframes: list[str] = []
        result: dict[str, Any] = {
            "batch_timestamp": current_time.isoformat(),
            "matched_timeframes": empty_timeframes,
            "total_configs": 0,
            "message": "å½“å‰æ—¶é—´æ— åŒ¹é…çš„æ—¶é—´å‘¨æœŸ",
        }
        return [], result

    return matched_timeframes, None


def _get_active_configs(
    db_manager: Any, matched_timeframes: list[str], current_time: datetime
) -> tuple[list[dict[str, Any]], dict[str, Any] | None]:
    """è·å–æ´»è·ƒé…ç½®"""
    active_configs = get_active_configs_by_timeframes(db_manager, matched_timeframes)
    if not active_configs:
        result = {
            "batch_timestamp": current_time.isoformat(),
            "matched_timeframes": matched_timeframes,
            "total_configs": 0,
            "message": "æ— æ´»è·ƒçš„äº¤æ˜“é…ç½®",
        }
        return [], result

    return active_configs, None


def _build_final_result(
    current_time: datetime,
    matched_timeframes: list[str],
    active_configs: list[dict[str, Any]],
    processing_results: list[dict[str, Any]],
) -> dict[str, Any]:
    """æ„å»ºæœ€ç»ˆç»“æœ"""
    result = {
        "batch_timestamp": current_time.isoformat(),
        "matched_timeframes": matched_timeframes,
        "total_configs": len(active_configs),
        "results": processing_results,
        "summary": {
            "total_processed": len(processing_results),
            "success_count": len(processing_results),
            "error_count": 0,
        },
    }

    final_msg = (
        f"æ•´ç‚¹ä»»åŠ¡å®Œæˆ: æ€»é…ç½®={len(active_configs)}, "
        f"æˆåŠŸ={len(processing_results)}, å¤±è´¥=0"
    )
    logger.info(final_msg)

    return result


async def execute_minute_task(db_manager: Any) -> dict[str, Any]:
    """æ‰§è¡Œæ¯åˆ†é’Ÿçš„æ•´ç‚¹ä»»åŠ¡ - çº¯å‡½æ•°å®ç°"""
    current_time = datetime.now(UTC)

    # 1. è·å–åŒ¹é…çš„æ—¶é—´å‘¨æœŸ
    matched_timeframes, early_result = _check_matched_timeframes(current_time)
    if early_result:
        return early_result

    # 2. æŸ¥è¯¢æ´»è·ƒé…ç½®
    active_configs, early_result = _get_active_configs(
        db_manager, matched_timeframes, current_time
    )
    if early_result:
        return early_result

    # 3. å¹¶å‘å¤„ç†æ‰€æœ‰é…ç½® - å¼‚å¸¸å‘ä¸Šä¼ æ’­, fail-fastç­–ç•¥
    processing_results = await process_configs_concurrent(active_configs)

    # 4. æ„å»ºå®Œæ•´ç»“æœ
    return _build_final_result(
        current_time, matched_timeframes, active_configs, processing_results
    )


def create_minute_task_handler(db_manager: Any) -> Any:
    """
    åˆ›å»ºåˆ†é’Ÿä»»åŠ¡å¤„ç†å™¨

    Args:
        db_manager: æ•°æ®åº“ç®¡ç†å™¨

    Returns:
        å¼‚æ­¥ä»»åŠ¡å¤„ç†å‡½æ•°
    """

    async def minute_task_handler() -> None:
        """æ¯åˆ†é’Ÿæ•´ç‚¹æ‰§è¡Œçš„ä»»åŠ¡å¤„ç†å™¨ - æ•è·å–æ¶ˆå¼‚å¸¸é¿å…è°ƒåº¦å™¨æŠ¥é”™"""
        try:
            _ = await execute_minute_task(db_manager)
        except asyncio.CancelledError:
            # é™é»˜å¤„ç†å–æ¶ˆå¼‚å¸¸, é¿å… apscheduler æŠ¥é”™
            return
        except Exception as e:
            logger.error("ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {}", str(e), exc_info=True)
            # å…¶ä»–å¼‚å¸¸ç»§ç»­å‘ä¸Šä¼ æ’­, ä¿æŒ fail-fast åŸåˆ™
            raise

    return minute_task_handler


def _setup_scheduler_and_task(db_manager: Any) -> tuple[Any, Any]:
    """åˆå§‹åŒ–è°ƒåº¦å™¨å’Œä»»åŠ¡"""
    scheduler = cast(Any, AsyncIOScheduler())
    task_handler = create_minute_task_handler(db_manager)

    scheduler.add_job(
        func=task_handler,
        trigger=CronTrigger(second=0),  # æ¯åˆ†é’Ÿçš„0ç§’æ‰§è¡Œ
        id="minute_order_builder_task",
        name="æ¯åˆ†é’Ÿè®¢å•æ„å»ºä»»åŠ¡",
        max_instances=1,  # é¿å…å¹¶å‘å åŠ æ‰§è¡Œ
        coalesce=True,  # åˆå¹¶é”™è¿‡çš„è§¦å‘,ä¸è¿½èµ¶è¡¥è·‘
        misfire_grace_time=30,  # è¶…è¿‡30ç§’åˆ™è·³è¿‡æœ¬æ¬¡
        replace_existing=True,
    )

    return scheduler, task_handler


async def _run_scheduler_loop(scheduler: Any) -> None:
    """è¿è¡Œè°ƒåº¦å™¨ä¸»å¾ªç¯"""
    scheduler.start()

    try:
        while True:
            await asyncio.sleep(1)
    except (KeyboardInterrupt, asyncio.CancelledError):
        await _shutdown_scheduler_gracefully(scheduler)


async def _shutdown_scheduler_gracefully(scheduler: Any) -> None:
    """ä¼˜é›…å…³é—­è°ƒåº¦å™¨"""
    try:
        scheduler.shutdown(wait=True)  # ç­‰å¾…æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆ
    except Exception:
        # å¦‚æœä¼˜é›…å…³é—­å¤±è´¥, å¼ºåˆ¶å…³é—­
        scheduler.shutdown(wait=False)

    # ç»™ä¸€ç‚¹æ—¶é—´è®©å¼‚æ­¥ä»»åŠ¡æ¸…ç†
    with contextlib.suppress(asyncio.CancelledError):
        await asyncio.sleep(0.1)

    # å…³é—­è¿›ç¨‹æ± 
    global _process_pool
    if _process_pool is not None:
        try:
            _process_pool.shutdown(wait=True, cancel_futures=True)
            logger.info("è¿›ç¨‹æ± å·²å…³é—­")
        except Exception:
            with contextlib.suppress(Exception):
                _process_pool.shutdown(wait=False, cancel_futures=True)
        finally:
            _process_pool = None


async def start_scheduler() -> None:
    """
    å¯åŠ¨è°ƒåº¦å™¨ - ä¸»å‡½æ•°

    è¿™æ˜¯ä¸€ä¸ªçº¯å‡½æ•°å¼çš„è°ƒåº¦å™¨å¯åŠ¨å™¨, ä¸ä½¿ç”¨ç±»
    """
    db_manager = get_database_manager(get_default_database_config())
    scheduler, _ = _setup_scheduler_and_task(db_manager)

    logger.info("æ•´ç‚¹è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    await _run_scheduler_loop(scheduler)


async def run_once_now(db_manager: Any = None) -> dict[str, Any]:
    """
    ç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡ - ç”¨äºæµ‹è¯•

    Args:
        db_manager: æ•°æ®åº“ç®¡ç†å™¨, å¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤çš„

    Returns:
        æ‰§è¡Œç»“æœ
    """
    if db_manager is None:
        db_manager = get_database_manager(get_default_database_config())

    logger.info("ç«‹å³æ‰§è¡Œä¸€æ¬¡æ•´ç‚¹ä»»åŠ¡æµ‹è¯•")
    result = await execute_minute_task(db_manager)
    return result


if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] in ["-h", "--help", "help"]:
            logger.info(__doc__)
            sys.exit(0)
        elif sys.argv[1] == "test":
            # æµ‹è¯•æ¨¡å¼ - ç«‹å³æ‰§è¡Œä¸€æ¬¡
            _ = asyncio.run(run_once_now())
            sys.exit(0)

    # æ­£å¸¸å¯åŠ¨è°ƒåº¦å™¨
    asyncio.run(start_scheduler())
